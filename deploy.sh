#!/bin/bash

# =================================================================
# è„šæœ¬åç§°: deploy.sh
# åŠŸèƒ½: å¯åŠ¨ vLLM API æœåŠ¡å¹¶æ‹‰èµ· Gradio Web UI
# ç”¨æ³•: sh deploy.sh --model_name <æ¨¡å‹ç›®å½•å> --api_port <ç«¯å£> --gui_port <ç«¯å£>
# =================================================================

MODEL_NAME=""
API_PORT=""
GUI_PORT=""

while [[ "$#" -gt 0 ]]; do
    case $1 in
        --model_name) MODEL_NAME="$2"; shift ;;
        --api_port) API_PORT="$2"; shift ;;
        --gui_port) GUI_PORT="$2"; shift ;;
        *) echo "âŒ æœªçŸ¥å‚æ•°: $1"; exit 1 ;;
    esac
    shift
done

if [ -z "$MODEL_NAME" ] || [ -z "$API_PORT" ] || [ -z "$GUI_PORT" ]; then
    echo "âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦å‚æ•°ã€‚"
    echo "ç”¨æ³•: sh deploy.sh --model_name Qwen/Qwen2.5-7B-Instruct --api_port 8000 --gui_port 7860"
    exit 1
fi

if [ -f "$(pwd)/python_env/bin/python" ]; then
    PYTHON_EXEC="$(pwd)/python_env/bin/python"
    export PATH="$(pwd)/python_env/bin:$PATH"
else
    PYTHON_EXEC="python"
fi

echo "========================================"
echo "æ­£åœ¨å¯åŠ¨éƒ¨ç½²æµç¨‹..."
echo "æ¨¡å‹è·¯å¾„: ./$MODEL_NAME"
echo "API ç«¯å£: $API_PORT"
echo "GUI ç«¯å£: $GUI_PORT"
echo "========================================"

if [ ! -d "./$MODEL_NAME" ]; then
    echo "âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹ç›®å½• ./$MODEL_NAME"
    echo "è¯·å…ˆä½¿ç”¨ download_model.sh ä¸‹è½½æ¨¡å‹ã€‚"
    exit 1
fi

cat > web_ui_launcher.py <<EOF
import gradio as gr
from openai import OpenAI
import time

client = OpenAI(
    base_url="http://localhost:${API_PORT}/v1",
    api_key="EMPTY"
)

def predict(message, history):
    history_openai_format = []
    for human, assistant in history:
        history_openai_format.append({"role": "user", "content": human})
        history_openai_format.append({"role": "assistant", "content": assistant})
    history_openai_format.append({"role": "user", "content": message})

    try:
        response = client.chat.completions.create(
            model="${MODEL_NAME}",
            messages=history_openai_format,
            stream=True,
            temperature=0.7
        )
        partial_message = ""
        for chunk in response:
            if chunk.choices[0].delta.content is not None:
                partial_message += chunk.choices[0].delta.content
                yield partial_message
    except Exception as e:
        yield f"âŒ Error: {str(e)}"

print("æ­£åœ¨å¯åŠ¨ Gradio ç•Œé¢ï¼Œç«¯å£: ${GUI_PORT}")
gr.ChatInterface(predict).launch(server_name="0.0.0.0", server_port=${GUI_PORT})
EOF

echo "ğŸš€ æ­£åœ¨å¯åŠ¨ vLLM API æœåŠ¡å™¨ (åå°è¿è¡Œ)..."
echo "æ—¥å¿—å°†è¾“å‡ºåˆ°: vllm_server.log"

nohup $PYTHON_EXEC -m vllm.entrypoints.openai.api_server \
    --model "./$MODEL_NAME" \
    --served-model-name "$MODEL_NAME" \
    --port "$API_PORT" \
    --trust-remote-code \
    --gpu-memory-utilization 0.9 > vllm_server.log 2>&1 &

VLLM_PID=$!

cleanup() {
    echo "æ­£åœ¨å…³é—­æœåŠ¡..."
    kill $VLLM_PID
    rm -f web_ui_launcher.py
    exit
}
trap cleanup SIGINT SIGTERM

echo "â³ ç­‰å¾…æ¨¡å‹åŠ è½½ (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)..."
while true; do
    if curl -s "http://localhost:${API_PORT}/v1/models" > /dev/null; then
        echo "âœ… vLLM API å·²å°±ç»ªï¼"
        break
    fi
    if ! ps -p $VLLM_PID > /dev/null; then
        echo "âŒ vLLM å¯åŠ¨å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ vllm_server.log"
        exit 1
    fi
    sleep 5
done

echo "ğŸš€ æ­£åœ¨å¯åŠ¨ Web UI..."
$PYTHON_EXEC web_ui_launcher.py